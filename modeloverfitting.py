# -*- coding: utf-8 -*-
"""Modeloverfitting.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KNzqXcgp-feZ6tynB2V5QUhOYdbCE89k

Syntheticdata

1500 instances- 0 or 1 binary problem

class 1 3 Gaussian distribution

class 0 - uniform distribution
"""



"""#synthetic data


Synthetic data -Synthetic data is any information manufactured artificially rather than generated by real world events. Algorithms create synthetic data used in model datasets for testing or training purposes. The synthetic data can mimic operational or production data and help train machine learning (ML) models or test out mathematical models.

Gaussian distribution

also known as normal distribution, is a continuous probability distribution that is widely used in statistical modeling and Machine Learning. It is a bell-shaped curve that is symmetrical around its mean and is characterized by its mean and standard deviation. The mean determines the center of the distribution, while the standard deviation determines the spread.

Uniform distribution

also known as the rectangular distribution, is a common distribution in machine learning. It has a flat shape that is equal across the entire range of values, which means that every value has the same probability of occurring. The uniform distribution is defined by two parameters: the minimum and maximum values, which determine the lower and upper bounds of the range.. The uniform distribution is defined by two parameters: the minimum and maximum values, which determine the lower and upper bounds of the range. The uniform distribution is often used to model random events or processes that have no preference or bias for any outcome, such as rolling a fair die, picking a card from a deck, or selecting a sample from a population. It has two main types: discrete uniform distribution there is a finite number of outcomes, each with an equal probability of occurrence and  continuous uniform distribution , deals with outcomes that are spread along a continuum. It is defined within a range [a, b], where all values between 'a' and 'b' are equally likely to occur.
"""

import numpy as np
import matplotlib.pyplot as plt

"""#matplotlibinline

"""

from numpy.random import random

N=1500

mean1=[6, 14]

mean2=[10,6]

mean3=[14,14]

cov= [3.5,0], [0, 3.5]

cov

np.random.seed(50)

X=np.random.multivariate_normal(mean1,cov, int(N/6))

X=np.random.multivariate_normal(mean2,cov, int(N/6))

X=np.random.multivariate_normal(mean3,cov, int(N/6))

X.shape

X[0]

"""# covariance

In mathematics and statistics, covariance is a measure of the relationship between two random variables. The metric evaluates how much – to what extent – the variables change together. In other words, it is essentially a measure of the variance between two variables. However, the metric does not assess the dependency between variables.

Multivariant normal distribution

The multivariate normal distribution is a generalization of the one-dimensional normal distribution to higher dimensions. In essence, it describes the probability distribution of a set of correlated random variables.
In a multivariate normal distribution, instead of having just one mean and one variance as in the one-dimensional case, you have a mean vector and a covariance matrix. The mean vector specifies the means of the individual variables, while the covariance matrix describes how the variables co-vary with each other.
"""

X[:,1].sum()

3506.3100408955534/250

X= np.concatenate((X, np.random.multivariate_normal(mean2, cov, int(N/6))))

X.shape

X= np.concatenate((X, np.random.multivariate_normal(mean3, cov, int(N/6))))

X.shape

X=np.concatenate((X, 20*np.random.rand(int(N/2), 2)))

X.shape

X

y = np.concatenate((np.ones(int(N/2)),np.zeros(int(N/2))))

y

plt.plot(X[:int(N/2), 0], X[:int(N/2),1], 'r+', X[int(N/2):,0],X[int(N/2):,1],'k.', ms=4)

# Training and test set creation

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test=train_test_split(X, y, train_size=0.8, random_state=11)

X_train.shape

X_test.shape

X_train[0]

y_train.shape

y_train

from sklearn import tree

maxDepth=[2,3,4,5,6,7,8,9,10,15,20,25,30,35,40,45,50]

len(maxDepth)

trainAcc=np.zeros(len(maxDepth))

trainAcc

testAcc=np.zeros(len(maxDepth))

testAcc

index=0

for depth in maxDepth:
  clf=tree.DecisionTreeClassifier(max_depth=depth)
  clf=clf.fit(X_train, y_train)
  y_predTrain=clf.predict(X_train)
  y_predTest=clf.predict(X_test)

  index +=1

""" trainAcc[index]=accuracy_score(y_train, y_predTrain)
  testAcc[index]=accuracy_score(y_test, y_predTest)
"""

testAcc

maxDepth

trainAcc

plt.plot(maxDepth, trainAcc, 'ro-', maxDepth, testAcc, 'bv--')
plt.legend(['Training Accuracy', 'Test Accuracy'])
plt.xlabel('Max Depth')
plt.ylabel('Accuracy')

print(type(X))
print(type(y))

print(X.shape)
print(y.shape)

print(X[:5])
print(y[:5])

X = np.ravel(X)
y = np.ravel(y)

from sklearn.linear_model import LinearRegression

linear_regression = LinearRegression()

linear_regression

linear_regression.fit(X=X_train,  y=y_train)

linear_regression.coef_

intercept = linear_regression.intercept_

linear_regression.intercept_

import seaborn as sns

axes = sns.scatterplot(x=X, y=y, palette='coolwarm', legend=False)
axes.set_ylim(0, 3)
x_range = np.array([min(x), max(x)])